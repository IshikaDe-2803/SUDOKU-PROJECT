{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "import numpy as np\r\n",
    "import cv2\r\n",
    "import operator\r\n",
    "import numpy as np\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "IMAGE_URL = \"./sudoku_1.jpg\"\r\n",
    "SUDOKU_SIZE = 9"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "def display_image(img):\r\n",
    "    cv2.imshow('Sudoku', img)\r\n",
    "    cv2.waitKey(0)  # Wait for any key to be pressed (with the image window active)\r\n",
    "    cv2.destroyAllWindows()  # Close all windows\r\n",
    "    return img"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "def preprocess(img, skip_dilate=False):\r\n",
    "    \"\"\"Uses a blurring function, adaptive thresholding and dilation to expose the main features of an image.\"\"\"\r\n",
    "    # Gaussian blur with a kernal size (height, width) of 9.\r\n",
    "    # Note that kernal sizes must be positive and odd and the kernel must be square.\r\n",
    "    #https://www.tutorialspoint.com/opencv/opencv_gaussian_blur.htm\r\n",
    "    blur_img = cv2.GaussianBlur(img.copy(), (9, 9), 0)\r\n",
    "    display_image(blur_img)\r\n",
    "\t# Adaptive threshold using 11 nearest neighbour pixels\r\n",
    "    # https://www.pyimagesearch.com/2021/05/12/adaptive-thresholding-with-opencv-cv2-adaptivethreshold/\r\n",
    "    # https://www.tutorialspoint.com/opencv/opencv_adaptive_threshold.htm\r\n",
    "    threshold_img = cv2.adaptiveThreshold(blur_img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\r\n",
    "    #display_image(threshold_img)\r\n",
    "    # Invert colours, so gridlines have non-zero pixel values.\r\n",
    "\t# Necessary to dilate the image, otherwise will look like erosion instead.\r\n",
    "    processed_img = cv2.bitwise_not(threshold_img, threshold_img)\r\n",
    "    #display_image(processed_img)\r\n",
    "\r\n",
    "    if not skip_dilate:\r\n",
    "    \t# Dilate the image to increase the size of the grid lines.\r\n",
    "    \tkernel = np.array([[0., 1., 0.], [1., 1., 1.], [0., 1., 0.]],np.uint8)\r\n",
    "    \tprocessed_img = cv2.dilate(processed_img, kernel)\r\n",
    "\r\n",
    "    return processed_img"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def find_corners_of_largest_polygon(img):\r\n",
    "\t\"\"\"Finds the 4 extreme corners of the largest contour in the image.\"\"\"\r\n",
    "\t#using copy of img as findcontours alters the original image\r\n",
    "\t# https://docs.opencv.org/master/d9/d8b/tutorial_py_contours_hierarchy.html\r\n",
    "\t\"\"\"The CHAIN_APPROX_SIMPLE  algorithm compresses horizontal, vertical, and diagonal segments along the \r\n",
    "\tcontour and leaves only their end points. This means that any of the points along the straight paths \r\n",
    "\twill be dismissed, and we will be left with only the end points. For example, consider a contour, \r\n",
    "\talong a rectangle. \r\n",
    "\tAll the contour points, except the four corner points will be dismissed. \r\n",
    "\t\"\"\"\r\n",
    "\tcontours, hierarchy = cv2.findContours(img.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)  # Find contours\r\n",
    "\tcontours = sorted(contours, key = cv2.contourArea, reverse = True)  # Sort by area, descending\r\n",
    "\tlargest_polygon = contours[0]  # Largest image\r\n",
    "\r\n",
    "\t# Use of `operator.itemgetter` with `max` and `min` allows us to get the index of the point\r\n",
    "\t# Each point is an array of 1 coordinate, hence the [0] getter, then [0] or [1] used to get x and y respectively.\r\n",
    "\r\n",
    "\t# Bottom-right point has the largest (x + y) value\r\n",
    "\t# Top-left has point smallest (x + y) value\r\n",
    "\t# Bottom-left point has smallest (x - y) value\r\n",
    "\t# Top-right point has largest (x - y) value\r\n",
    "\tbottom_right = max(enumerate([point[0][0] + point[0][1] for point in largest_polygon]), key = operator.itemgetter(1))[0]\r\n",
    "\ttop_left = min(enumerate([point[0][0] + point[0][1] for point in largest_polygon]), key = operator.itemgetter(1))[0]\r\n",
    "\tbottom_left = min(enumerate([point[0][0] - point[0][1] for point in largest_polygon]), key = operator.itemgetter(1))[0]\r\n",
    "\ttop_right = max(enumerate([point[0][0] - point[0][1] for point in largest_polygon]), key = operator.itemgetter(1))[0]\r\n",
    "\r\n",
    "\t# Return an array of all 4 points using the indices\r\n",
    "\t# Each point is in its own array of one coordinate\r\n",
    "\t#Order: TL, TR, BR, BL (CLOCKWISE)\r\n",
    "\treturn [largest_polygon[top_left][0], largest_polygon[top_right][0], largest_polygon[bottom_right][0], largest_polygon[bottom_left][0]]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "def distance(point1, point2):\r\n",
    "\t\"\"\"Returns the scalar distance between two points\"\"\"\r\n",
    "\ta = point2[0] - point1[0]\r\n",
    "\tb = point2[1] - point1[1]\r\n",
    "\treturn np.sqrt((a ** 2) + (b ** 2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "def crop_and_warp(img, corners):\r\n",
    "\t\"\"\"Crops and warps a rectangular section from an image into a square of similar size.\"\"\"\r\n",
    "\r\n",
    "\t# Rectangle described by top left, top right, bottom right and bottom left points\r\n",
    "\ttop_left, top_right, bottom_right, bottom_left = corners[0], corners[1], corners[2], corners[3]\r\n",
    "\r\n",
    "\t# Explicitly set the data type to float32 or `getPerspectiveTransform` will throw an error\r\n",
    "\timg_float32 = np.array([top_left, top_right, bottom_right, bottom_left], dtype='float32')\r\n",
    "\r\n",
    "\t# Get the longest side in the rectangle\r\n",
    "\tside_length = max([\r\n",
    "\t\tdistance(bottom_right, top_right),\r\n",
    "\t\tdistance(top_left, bottom_left),\r\n",
    "\t\tdistance(bottom_right, bottom_left),\r\n",
    "\t\tdistance(top_left, top_right)\r\n",
    "\t])\r\n",
    "\t#https://theailearner.com/tag/cv2-getperspectivetransform/\r\n",
    "\t#https://www.pyimagesearch.com/2014/08/25/4-point-opencv-getperspective-transform-example/\r\n",
    "\t# Describe a square with side of the calculated length, this is the new perspective we want to warp to\r\n",
    "\tnew_perspective = np.array([[0, 0], [side_length - 1, 0], [side_length - 1, side_length - 1], [0, side_length - 1]], dtype='float32')\r\n",
    "\r\n",
    "\t# Gets the transformation matrix for skewing the image to fit a square by comparing the 4 before and after points\r\n",
    "\ttransformed_img = cv2.getPerspectiveTransform(img_float32, new_perspective)\r\n",
    "\t# Performs the transformation on the original image\r\n",
    "\treturn cv2.warpPerspective(img, transformed_img, (int(side_length), int(side_length)))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "def all_cell_coordinates(img):\r\n",
    "\t\"\"\"Infers 81 cell grid from a square image.\"\"\"\r\n",
    "\tsquares = []\r\n",
    "\tside = img.shape[:1]\r\n",
    "\tside = side[0] / SUDOKU_SIZE\r\n",
    "\r\n",
    "\t# Note that we swap j and i here so the rectangles are stored in the list reading left-right instead of top-down.\r\n",
    "\tfor j in range(SUDOKU_SIZE):\r\n",
    "\t\tfor i in range(SUDOKU_SIZE):\r\n",
    "\t\t\tp1 = (i * side, j * side)  # Top left corner of a bounding box\r\n",
    "\t\t\tp2 = ((i + 1) * side, (j + 1) * side)  # Bottom right corner of bounding box\r\n",
    "\t\t\tsquares.append((p1, p2))\r\n",
    "\treturn squares"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "def get_digit_cell(img, cell):\r\n",
    "\t\"\"\"Cuts a rectangle from an image using the top left and bottom right points.\"\"\"\r\n",
    "\treturn img[int(cell[0][1]):int(cell[1][1]), int(cell[0][0]):int(cell[1][0])]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "def find_largest_feature(inp_img, scan_tl=None, scan_br=None):\r\n",
    "    \"\"\"\r\n",
    "    Uses the fact the `floodFill` function returns a bounding box of the area it filled to find the biggest\r\n",
    "    connected pixel structure in the image. Fills this structure in white, reducing the rest to black.\r\n",
    "    \"\"\"\r\n",
    "    img = inp_img.copy()  # Copy the image, leaving the original untouched\r\n",
    "    height, width = img.shape[:2]\r\n",
    "\r\n",
    "    max_area = 0\r\n",
    "    seed_point = (None, None)\r\n",
    "\r\n",
    "    if scan_tl is None:\r\n",
    "        scan_tl = [0, 0]\r\n",
    "\r\n",
    "    if scan_br is None:\r\n",
    "        scan_br = [width, height]\r\n",
    "\r\n",
    "    # Loop through the image\r\n",
    "    for x in range(scan_tl[0], scan_br[0]):\r\n",
    "        for y in range(scan_tl[1], scan_br[1]):\r\n",
    "            # Only operate on light or white squares\r\n",
    "            # Note that .item() appears to take input as y, x\r\n",
    "            if img.item(y, x) == 255 and x < width and y < height:\r\n",
    "                area = cv2.floodFill(img, None, (x, y), 64)\r\n",
    "                if area[0] > max_area:  # Gets the maximum bound area which should be the grid\r\n",
    "                    max_area = area[0]\r\n",
    "                    seed_point = (x, y)\r\n",
    "\r\n",
    "    # Colour everything grey (compensates for features outside of our middle scanning range\r\n",
    "    for x in range(width):\r\n",
    "        for y in range(height):\r\n",
    "            if img.item(y, x) == 255 and x < width and y < height:\r\n",
    "                cv2.floodFill(img, None, (x, y), 64)\r\n",
    "\r\n",
    "    # Mask that is 2 pixels bigger than the image\r\n",
    "    mask = np.zeros((height + 2, width + 2), np.uint8)\r\n",
    "\r\n",
    "    # Highlight the main feature\r\n",
    "    if all([p is not None for p in seed_point]):\r\n",
    "        cv2.floodFill(img, mask, seed_point, 255)\r\n",
    "\r\n",
    "    top, bottom, left, right = height, 0, width, 0\r\n",
    "\r\n",
    "    for x in range(width):\r\n",
    "        for y in range(height):\r\n",
    "            if img.item(y, x) == 64:  # Hide anything that isn't the main feature\r\n",
    "                cv2.floodFill(img, mask, (x, y), 0)\r\n",
    "\r\n",
    "            # Find the bounding parameters\r\n",
    "            if img.item(y, x) == 255:\r\n",
    "                top = y if y < top else top\r\n",
    "                bottom = y if y > bottom else bottom\r\n",
    "                left = x if x < left else left\r\n",
    "                right = x if x > right else right\r\n",
    "\r\n",
    "    bbox = [[left, top], [right, bottom]]\r\n",
    "    return img, np.array(bbox, dtype='float32'), seed_point"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "def scale_and_centre(img, size, margin=0, background=0):\r\n",
    "    \"\"\"Scales and centres an image onto a new background square.\"\"\"\r\n",
    "    h, w = img.shape[:2]\r\n",
    "\r\n",
    "    def centre_pad(length):\r\n",
    "        \"\"\"Handles centering for a given length that may be odd or even.\"\"\"\r\n",
    "        if length % 2 == 0:\r\n",
    "            side1 = int((size - length) / 2)\r\n",
    "            side2 = side1\r\n",
    "        else:\r\n",
    "            side1 = int((size - length) / 2)\r\n",
    "            side2 = side1 + 1\r\n",
    "        return side1, side2\r\n",
    "\r\n",
    "    def scale(r, x):\r\n",
    "        return int(r * x)\r\n",
    "\r\n",
    "    if h > w:\r\n",
    "        t_pad = int(margin / 2)\r\n",
    "        b_pad = t_pad\r\n",
    "        ratio = (size - margin) / h\r\n",
    "        w, h = scale(ratio, w), scale(ratio, h)\r\n",
    "        l_pad, r_pad = centre_pad(w)\r\n",
    "    else:\r\n",
    "        l_pad = int(margin / 2)\r\n",
    "        r_pad = l_pad\r\n",
    "        ratio = (size - margin) / w\r\n",
    "        w, h = scale(ratio, w), scale(ratio, h)\r\n",
    "        t_pad, b_pad = centre_pad(h)\r\n",
    "\r\n",
    "    img = cv2.resize(img, (w, h))\r\n",
    "    img = cv2.copyMakeBorder(img, t_pad, b_pad, l_pad,\r\n",
    "                             r_pad, cv2.BORDER_CONSTANT, None, background)\r\n",
    "    return cv2.resize(img, (size, size))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "def extract_digit(img, cell, size):\r\n",
    "\t\"\"\"Extracts a digit (if one exists) from a Sudoku square.\"\"\"\r\n",
    "\r\n",
    "\tdigit = get_digit_cell(img, cell)  # Get the digit box from the whole square\r\n",
    "\r\n",
    "\t# Use fill feature finding to get the largest feature in middle of the box\r\n",
    "\t# Margin used to define an area in the middle we would expect to find a pixel belonging to the digit\r\n",
    "\theight, width = digit.shape[:2]\r\n",
    "\tmargin = int(np.mean([height, width]) / 2.5) ##NO IDEA WHY DIV BY 2.5\r\n",
    "\t_, bbox, seed = find_largest_feature(digit, [margin, margin], [width - margin, height - margin])\r\n",
    "\tdigit = get_digit_cell(digit, bbox)\r\n",
    "\r\n",
    "\t# Scale and pad the digit so that it fits a square of the digit size we're using for machine learning\r\n",
    "\twidth = bbox[1][0] - bbox[0][0]\r\n",
    "\theight = bbox[1][1] - bbox[0][1]\r\n",
    "\r\n",
    "\t# Ignore any small bounding boxes\r\n",
    "\tif width > 0 and height > 0 and (width * height) > 100 and len(digit) > 0:\r\n",
    "\t\treturn scale_and_centre(digit, size, 4)\r\n",
    "\telse:\r\n",
    "\t\treturn np.zeros((size, size), np.uint8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "def get_digits(img, cell_coordinates, size):\r\n",
    "    \"\"\"Extracts digits from their cells and builds an array\"\"\"\r\n",
    "    digits = []\r\n",
    "    img = preprocess(img.copy(), skip_dilate=True)\r\n",
    "    #cv2.imshow('img', img)\r\n",
    "    for cell in cell_coordinates:\r\n",
    "        digits.append(extract_digit(img, cell, size))\r\n",
    "    return digits"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "def show_digits(digits, colour=255):\r\n",
    "    \"\"\"Shows list of 81 extracted digits in a grid format\"\"\"\r\n",
    "    grid = []\r\n",
    "    with_border = [cv2.copyMakeBorder(img.copy(), 1, 1, 1, 1, cv2.BORDER_CONSTANT, None, colour) for img in digits]\r\n",
    "    for i in range(SUDOKU_SIZE):\r\n",
    "        row = np.concatenate(with_border[i * SUDOKU_SIZE:((i + 1) * SUDOKU_SIZE)], axis=1)\r\n",
    "        grid.append(row)\r\n",
    "    img = display_image(np.concatenate(grid))\r\n",
    "    return img"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "def extract_sudoku(path):\r\n",
    "    original_img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\r\n",
    "    display_image(original_img)\r\n",
    "    processed_img = preprocess(original_img)\r\n",
    "    corners = find_corners_of_largest_polygon(processed_img)\r\n",
    "    #print(corners)\r\n",
    "    cropped = crop_and_warp(original_img, corners)\r\n",
    "    #display_image(cropped)    \r\n",
    "    cell_coordinates = all_cell_coordinates(cropped)\r\n",
    "    print(cell_coordinates)\r\n",
    "    # digits = get_digits(cropped, cell_coordinates, 28)\r\n",
    "    # print(digits)\r\n",
    "    # final_image = show_digits(digits)\r\n",
    "    # return final_image\r\n",
    "extract_sudoku(IMAGE_URL)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[((0.0, 0.0), (38.111111111111114, 38.111111111111114)), ((38.111111111111114, 0.0), (76.22222222222223, 38.111111111111114)), ((76.22222222222223, 0.0), (114.33333333333334, 38.111111111111114)), ((114.33333333333334, 0.0), (152.44444444444446, 38.111111111111114)), ((152.44444444444446, 0.0), (190.55555555555557, 38.111111111111114)), ((190.55555555555557, 0.0), (228.66666666666669, 38.111111111111114)), ((228.66666666666669, 0.0), (266.7777777777778, 38.111111111111114)), ((266.7777777777778, 0.0), (304.8888888888889, 38.111111111111114)), ((304.8888888888889, 0.0), (343.0, 38.111111111111114)), ((0.0, 38.111111111111114), (38.111111111111114, 76.22222222222223)), ((38.111111111111114, 38.111111111111114), (76.22222222222223, 76.22222222222223)), ((76.22222222222223, 38.111111111111114), (114.33333333333334, 76.22222222222223)), ((114.33333333333334, 38.111111111111114), (152.44444444444446, 76.22222222222223)), ((152.44444444444446, 38.111111111111114), (190.55555555555557, 76.22222222222223)), ((190.55555555555557, 38.111111111111114), (228.66666666666669, 76.22222222222223)), ((228.66666666666669, 38.111111111111114), (266.7777777777778, 76.22222222222223)), ((266.7777777777778, 38.111111111111114), (304.8888888888889, 76.22222222222223)), ((304.8888888888889, 38.111111111111114), (343.0, 76.22222222222223)), ((0.0, 76.22222222222223), (38.111111111111114, 114.33333333333334)), ((38.111111111111114, 76.22222222222223), (76.22222222222223, 114.33333333333334)), ((76.22222222222223, 76.22222222222223), (114.33333333333334, 114.33333333333334)), ((114.33333333333334, 76.22222222222223), (152.44444444444446, 114.33333333333334)), ((152.44444444444446, 76.22222222222223), (190.55555555555557, 114.33333333333334)), ((190.55555555555557, 76.22222222222223), (228.66666666666669, 114.33333333333334)), ((228.66666666666669, 76.22222222222223), (266.7777777777778, 114.33333333333334)), ((266.7777777777778, 76.22222222222223), (304.8888888888889, 114.33333333333334)), ((304.8888888888889, 76.22222222222223), (343.0, 114.33333333333334)), ((0.0, 114.33333333333334), (38.111111111111114, 152.44444444444446)), ((38.111111111111114, 114.33333333333334), (76.22222222222223, 152.44444444444446)), ((76.22222222222223, 114.33333333333334), (114.33333333333334, 152.44444444444446)), ((114.33333333333334, 114.33333333333334), (152.44444444444446, 152.44444444444446)), ((152.44444444444446, 114.33333333333334), (190.55555555555557, 152.44444444444446)), ((190.55555555555557, 114.33333333333334), (228.66666666666669, 152.44444444444446)), ((228.66666666666669, 114.33333333333334), (266.7777777777778, 152.44444444444446)), ((266.7777777777778, 114.33333333333334), (304.8888888888889, 152.44444444444446)), ((304.8888888888889, 114.33333333333334), (343.0, 152.44444444444446)), ((0.0, 152.44444444444446), (38.111111111111114, 190.55555555555557)), ((38.111111111111114, 152.44444444444446), (76.22222222222223, 190.55555555555557)), ((76.22222222222223, 152.44444444444446), (114.33333333333334, 190.55555555555557)), ((114.33333333333334, 152.44444444444446), (152.44444444444446, 190.55555555555557)), ((152.44444444444446, 152.44444444444446), (190.55555555555557, 190.55555555555557)), ((190.55555555555557, 152.44444444444446), (228.66666666666669, 190.55555555555557)), ((228.66666666666669, 152.44444444444446), (266.7777777777778, 190.55555555555557)), ((266.7777777777778, 152.44444444444446), (304.8888888888889, 190.55555555555557)), ((304.8888888888889, 152.44444444444446), (343.0, 190.55555555555557)), ((0.0, 190.55555555555557), (38.111111111111114, 228.66666666666669)), ((38.111111111111114, 190.55555555555557), (76.22222222222223, 228.66666666666669)), ((76.22222222222223, 190.55555555555557), (114.33333333333334, 228.66666666666669)), ((114.33333333333334, 190.55555555555557), (152.44444444444446, 228.66666666666669)), ((152.44444444444446, 190.55555555555557), (190.55555555555557, 228.66666666666669)), ((190.55555555555557, 190.55555555555557), (228.66666666666669, 228.66666666666669)), ((228.66666666666669, 190.55555555555557), (266.7777777777778, 228.66666666666669)), ((266.7777777777778, 190.55555555555557), (304.8888888888889, 228.66666666666669)), ((304.8888888888889, 190.55555555555557), (343.0, 228.66666666666669)), ((0.0, 228.66666666666669), (38.111111111111114, 266.7777777777778)), ((38.111111111111114, 228.66666666666669), (76.22222222222223, 266.7777777777778)), ((76.22222222222223, 228.66666666666669), (114.33333333333334, 266.7777777777778)), ((114.33333333333334, 228.66666666666669), (152.44444444444446, 266.7777777777778)), ((152.44444444444446, 228.66666666666669), (190.55555555555557, 266.7777777777778)), ((190.55555555555557, 228.66666666666669), (228.66666666666669, 266.7777777777778)), ((228.66666666666669, 228.66666666666669), (266.7777777777778, 266.7777777777778)), ((266.7777777777778, 228.66666666666669), (304.8888888888889, 266.7777777777778)), ((304.8888888888889, 228.66666666666669), (343.0, 266.7777777777778)), ((0.0, 266.7777777777778), (38.111111111111114, 304.8888888888889)), ((38.111111111111114, 266.7777777777778), (76.22222222222223, 304.8888888888889)), ((76.22222222222223, 266.7777777777778), (114.33333333333334, 304.8888888888889)), ((114.33333333333334, 266.7777777777778), (152.44444444444446, 304.8888888888889)), ((152.44444444444446, 266.7777777777778), (190.55555555555557, 304.8888888888889)), ((190.55555555555557, 266.7777777777778), (228.66666666666669, 304.8888888888889)), ((228.66666666666669, 266.7777777777778), (266.7777777777778, 304.8888888888889)), ((266.7777777777778, 266.7777777777778), (304.8888888888889, 304.8888888888889)), ((304.8888888888889, 266.7777777777778), (343.0, 304.8888888888889)), ((0.0, 304.8888888888889), (38.111111111111114, 343.0)), ((38.111111111111114, 304.8888888888889), (76.22222222222223, 343.0)), ((76.22222222222223, 304.8888888888889), (114.33333333333334, 343.0)), ((114.33333333333334, 304.8888888888889), (152.44444444444446, 343.0)), ((152.44444444444446, 304.8888888888889), (190.55555555555557, 343.0)), ((190.55555555555557, 304.8888888888889), (228.66666666666669, 343.0)), ((228.66666666666669, 304.8888888888889), (266.7777777777778, 343.0)), ((266.7777777777778, 304.8888888888889), (304.8888888888889, 343.0)), ((304.8888888888889, 304.8888888888889), (343.0, 343.0))]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}